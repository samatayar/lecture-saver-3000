# -*- coding: utf-8 -*-
"""Finallllllll

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cq_SLhvjqHkgUQ1oQ6VJWfdfoNBFMc36

## âš™ï¸ Setup and Installation

First, we need to install the necessary libraries.
- `chromadb`: Our database to store text as numbers.
- `google-generativeai`: To talk to the Gemini LLM.
- `sentence-transformers`: To convert text into embeddings (vectors).
"""

!pip install docling
!pip install langchain
!pip install langchain-text-splitters
!pip install langchain-core
!pip install unstructured
!pip install llama_index
!pip install -qU langchain-text-splitters
!pip install sentence-transformers
!pip install faiss-cpu
!pip install openai
!pip install chromadb

"""## ğŸ” Configuration

We need to set up our access to Google's Gemini models.

**Instructions:**
1. Go to [Openrouter](https://openrouter.ai/keys).
2. Create an API Key.
3. Paste it below.
"""

import os
os.environ["OPENROUTER_API_KEY"] = "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"

from huggingface_hub import login

login("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")

"""## ğŸ“‚ Step 1: Extract and clean  Data (using docling lib)"""

from pathlib import Path
import re
from docling.document_converter import DocumentConverter


def extract_pdf_to_markdown(pdf_path: str) -> str | None:
    pdf_file = Path(pdf_path)

    if not pdf_file.exists() or not pdf_file.is_file():
        print(f"File not found or not a file: {pdf_path}")
        return None

    print(f"Processing: {pdf_file.name}")

    try:
        converter = DocumentConverter()
        result = converter.convert(str(pdf_file))
        doc = result.document

        raw_md = doc.export_to_markdown(
            page_break_placeholder="\n\n---\n\n"
        )

        cleaned = re.sub(r'<!--.*?-->', '', raw_md, flags=re.DOTALL)
        cleaned = re.sub(r'</?image[^>]*>', '', cleaned, flags=re.IGNORECASE)
        cleaned = re.sub(r'\n{3,}', '\n\n', cleaned)
        cleaned = re.sub(r'\s*---\s*', '\n\n---\n\n', cleaned)
        cleaned = cleaned.strip()

        output_file = pdf_file.parent / f"{pdf_file.stem}_markdown.md"
        output_file.write_text(cleaned, encoding='utf-8')

        print(f"Saved to: {output_file}")
        print(f"Content length: {len(cleaned)} characters")

        return str(output_file)

    except Exception as e:
        print(f"Error: {str(e)}")
        return None

"""###To use fun"""

result = extract_pdf_to_markdown("/content/TNN lec-1.pdf")

"""## âœ‚ï¸ Step 2: Chunking (Splitting)

"""

from pathlib import Path
import json
import re
from typing import List

from langchain_text_splitters import MarkdownHeaderTextSplitter, RecursiveCharacterTextSplitter

class Document:
    def __init__(self, page_content: str, metadata: dict = None):
        self.page_content = page_content
        self.metadata = metadata or {}


def describe_tables_and_figures(text: str) -> str:
    md_table_pattern = re.compile(r"(\|.+\|\n\|[-:\s|]+\|\n(?:\|.*\|\n?)*)", re.MULTILINE)
    def md_table_to_text(match):
        table = match.group(1)
        rows = [r.strip() for r in table.splitlines() if r.strip()]
        header = rows[0] if rows else ""
        data_rows = rows[2:] if len(rows) > 2 else []
        return f"\n[Ø¬Ø¯ÙˆÙ„]: ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†Ø¸Ù…Ø©. Ø§Ù„Ø¹Ù†Ø§ÙˆÙŠÙ†: {header}. Ø¹Ø¯Ø¯ Ø§Ù„ØµÙÙˆÙ: {len(data_rows)}.\n"
    text = md_table_pattern.sub(md_table_to_text, text)
    text = re.sub(r"<table.*?>.*?</table>", "\n[Ø¬Ø¯ÙˆÙ„]: Ø¬Ø¯ÙˆÙ„ HTML ÙŠØ­ØªÙˆÙŠ Ø¹Ù„Ù‰ Ø¨ÙŠØ§Ù†Ø§Øª Ù…Ù†Ø¸Ù…Ø©.\n", text, flags=re.DOTALL|re.IGNORECASE)
    text = re.sub(r"```mermaid.*?```", "\n[Ù…Ø®Ø·Ø·]: Ù…Ø®Ø·Ø· Mermaid ÙŠÙˆØ¶Ø­ Ø¹Ù„Ø§Ù‚Ø§Øª Ø£Ùˆ ØªØ¯ÙÙ‚ Ø¹Ù…Ù„ÙŠØ§Øª.\n", text, flags=re.DOTALL)
    text = re.sub(r"!\[.*?\]\(.*?\)", "\n[Ø´ÙƒÙ„]: Ø´ÙƒÙ„ Ø£Ùˆ Ù…Ø®Ø·Ø· Ø¨ØµØ±ÙŠ.\n", text)
    return text


def merge_empty_sections(docs: List[Document]) -> List[Document]:
    merged_docs = []
    buffer_doc = None
    for doc in docs:
        content = doc.page_content.strip()
        if not content:
            if buffer_doc is not None:
                buffer_doc.page_content += "\n" + content
            else:
                buffer_doc = doc
            continue
        if buffer_doc is not None:
            buffer_doc.page_content += "\n" + content
            merged_docs.append(buffer_doc)
            buffer_doc = None
        else:
            merged_docs.append(doc)
    if buffer_doc is not None:
        merged_docs.append(buffer_doc)
    return merged_docs


def split_md_files_to_json(
    md_files: List[str],
    output_dir: str,
    chunk_size: int = 900,
    chunk_overlap: int = 120
):
    output_path = Path(output_dir)
    output_path.mkdir(parents=True, exist_ok=True)

    header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=[("#", "h1"), ("##", "h2"), ("###", "h3")])
    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)

    for md_file in md_files:
        md_path = Path(md_file)
        raw_text = md_path.read_text(encoding="utf-8")
        pages = raw_text.split("\n---\n")
        all_chunks = []

        for page_num, page_text in enumerate(pages, start=1):
            clean_text = describe_tables_and_figures(page_text)
            header_docs = header_splitter.split_text(clean_text)
            header_docs = merge_empty_sections(header_docs)

            for doc in header_docs:
                header = doc.metadata.get("h3") or doc.metadata.get("h2") or doc.metadata.get("h1")
                chunks = text_splitter.split_text(doc.page_content)
                for chunk in chunks:
                    all_chunks.append({
                        "text": chunk.strip(),
                        "metadata": {
                            "source": md_path.name,
                            "page": page_num,
                            "header": header,
                        }
                    })

        out_file = output_path / f"{md_path.stem}.json"
        out_file.write_text(json.dumps(all_chunks, ensure_ascii=False, indent=2), encoding="utf-8")

"""### To use fun"""

from pathlib import Path


md_folder = Path("/content/")
md_files = list(md_folder.glob("*.md"))
md_files = [str(p) for p in md_files]
output_dir = "/content/FinalChunk"

split_md_files_to_json(md_files, output_dir)

"""## ğŸ”¢ Step 3: Embeddings (Vectorization)

"""

from pathlib import Path
import json
import re
from sentence_transformers import SentenceTransformer
import numpy as np

def clean_text(text: str) -> str:
    text = re.sub(r'```.*?```', ' ', text, flags=re.DOTALL)
    text = re.sub(r'`([^`]+)`', r'\1', text)
    text = re.sub(r'#{1,6}\s+', '', text)
    text = re.sub(r'^\s*>\s*', '', text, flags=re.MULTILINE)
    text = re.sub(r'^\s*[\*\-\+]\s+', '', text, flags=re.MULTILINE)
    text = re.sub(r'\*\*([^\*]+)\*\*', r'\1', text)
    text = re.sub(r'\*([^\*]+)\*', r'\1', text)
    text = re.sub(r'__([^_]+)__', r'\1', text)
    text = re.sub(r'_([^_]+)_', r'\1', text)
    text = re.sub(r'\[([^\]]+)\]\([^\)]+\)', r'\1', text)
    text = re.sub(r'https?://[^\s]+', '', text)
    text = re.sub(r'<[^>]+>', '', text)
    text = re.sub(r'\|[\s\-\:]+\|', ' ', text)
    text = re.sub(r'\|', ' ', text)
    text = re.sub(r'([!?.]){3,}', r'\1', text)
    text = re.sub(r'\n{3,}', '\n\n', text)
    text = '\n'.join(line.strip() for line in text.split('\n'))
    text = re.sub(r'\s+', ' ', text)
    return text.strip()

def generate_embeddings(input_path: str | Path, output_dir: str | Path, model_name: str = "sentence-transformers/distiluse-base-multilingual-cased-v2"):
    input_path = Path(input_path)
    output_dir = Path(output_dir)
    output_dir.mkdir(parents=True, exist_ok=True)

    model = SentenceTransformer(model_name)

    json_files = [input_path] if input_path.is_file() else list(input_path.glob("*.json"))

    for json_file in json_files:
        with open(json_file, encoding="utf-8") as f:
            chunks = json.load(f)

        results = []
        for i, chunk in enumerate(chunks, start=1):
            text = chunk.get("text") or chunk.get("text_content", "")
            text = clean_text(text)
            if not text:
                continue

            vector = model.encode(text, convert_to_numpy=True)
            vector = vector.astype(float).tolist()

            results.append({
                "id": f"{json_file.stem}_chunk_{i}",
                "vector": vector,
                "text_content": text,
                "metadata": chunk.get("metadata", {})
            })

        out_file = output_dir / f"{json_file.stem}_embeddings.json"
        with open(out_file, "w", encoding="utf-8") as f:
            json.dump(results, f, ensure_ascii=False, indent=2)

        print(f"{len(results)} embeddings saved in {out_file}")

    print("Embeddings generation completed")

"""### To use fun"""

generate_embeddings(
    input_path="/content/FinalChunk",
    output_dir="/content/FinalEmbeddings"
)

"""## ğŸ’¾ Step 4: Indexing (Vector Database)

Now we need a place to store these vectors so we can search them quickly.
We will use **ChromaDB**. It is an open-source vector database.
"""

import chromadb
from pathlib import Path
import json


def clean_metadata(metadata: dict) -> dict:
    return {k: ("/" if v is None else v) for k, v in metadata.items()}


def load_embeddings_to_chroma(
    embeddings_path: str | Path,
    chroma_db_path: str | Path = "./chromaDB",
    collection_name: str = "Final",
    reset_collection: bool = True,
    peek_examples: int = 5
):
    embeddings_path = Path(embeddings_path)

    client = chromadb.PersistentClient(path=str(chroma_db_path))

    if reset_collection:
        try:
            client.delete_collection(collection_name)
        except Exception:
            pass

    collection = client.get_or_create_collection(name=collection_name)

    all_chunks = []
    for json_file in embeddings_path.glob("*.json"):
        with open(json_file, "r", encoding="utf-8") as f:
            chunks = json.load(f)

        for chunk in chunks:
            chunk.setdefault("metadata", {})
            chunk["metadata"].setdefault("source", json_file.name)
            all_chunks.append(chunk)

    print(f"Found {len(all_chunks)} chunks")

    for chunk in all_chunks:
        collection.add(
            ids=[chunk["id"]],
            documents=[chunk["text_content"]],
            embeddings=[chunk["vector"]],
            metadatas=[clean_metadata(chunk["metadata"])]
        )

    print(f"Stored {collection.count()} vectors in Chroma")

    if peek_examples > 0:
        results = collection.peek(peek_examples)
        for i, cid in enumerate(results["ids"]):
            print(f"\nExample {i + 1}")
            print("ID:", cid)
            print("Text:", results["documents"][i][:300], "...")
            print("Source:", results["metadatas"][i].get("source"))
            print("Page:", results["metadatas"][i].get("page"))
            print("Header:", results["metadatas"][i].get("header"))

    return collection

"""### To use fun"""

collection = load_embeddings_to_chroma(
    embeddings_path="/content/FinalEmbeddings",
    chroma_db_path="./chromaDB",
    collection_name="Final",
    reset_collection=True
)

import chromadb
import json

def export_chroma_to_json(db_path="./chromaDB", collection_name="Final", output_dir="./exported_json"):
    import os
    os.makedirs(output_dir, exist_ok=True)

    client = chromadb.PersistentClient(path=db_path)
    collection = client.get_collection(collection_name)

    # Ù†Ø³Ø­Ø¨ ÙƒÙ„ Ø§Ù„ÙˆØ«Ø§Ø¦Ù‚ Ù…Ù† Ø§Ù„ÙƒÙˆÙ„ÙŠÙƒØ´Ù†
    all_docs = collection.get(include=["documents", "metadatas", "ids"])

    chunks_data = []
    sources_data = []

    for doc, meta, doc_id in zip(all_docs["documents"], all_docs["metadatas"], all_docs["ids"]):
        for i, text in enumerate(doc):
            chunk_entry = {
                "id": f"{doc_id}_chunk_{i+1}",
                "text_content": text,
                "metadata": meta[i] if i < len(meta) else {}
            }
            chunks_data.append(chunk_entry)

            # Ø­ÙØ¸ Ø§Ù„Ù…ØµØ§Ø¯Ø± ÙÙ‚Ø·
            source_entry = {
                "id": chunk_entry["id"],
                "source": meta[i].get("source") if i < len(meta) else None,
                "page": meta[i].get("page") if i < len(meta) else None
            }
            sources_data.append(source_entry)

    # Ø­ÙØ¸ JSON Ù„Ù„Ù€ chunks
    chunks_file = os.path.join(output_dir, "all_chunks.json")
    with open(chunks_file, "w", encoding="utf-8") as f:
        json.dump(chunks_data, f, ensure_ascii=False, indent=2)

    # Ø­ÙØ¸ JSON Ù„Ù„Ù…ØµØ§Ø¯Ø±
    sources_file = os.path.join(output_dir, "sources_and_pages.json")
    with open(sources_file, "w", encoding="utf-8") as f:
        json.dump(sources_data, f, ensure_ascii=False, indent=2)

    print(f"Saved {len(chunks_data)} chunks to {chunks_file}")
    print(f"Saved sources info to {sources_file}")

"""## ğŸ” Step 5: Retrieval"""

import chromadb
from sentence_transformers import SentenceTransformer

def query_chroma(
    query: str,
    db_path: str = "./chromaDB",
    collection_name: str = "Final",
    model_name: str = "sentence-transformers/distiluse-base-multilingual-cased-v2",
    n_results: int = 10,
    where: dict | None = None
):
    client = chromadb.PersistentClient(path=db_path)
    collection = client.get_collection(collection_name)

    model = SentenceTransformer(model_name)
    query_vector = model.encode([query], convert_to_numpy=True)[0].astype(float).tolist()

    results = collection.query(
        query_embeddings=[query_vector],
        n_results=n_results,
        where=where
    )

    output = []
    for i, doc in enumerate(results["documents"][0]):
        output.append({
            "text": doc,
            "metadata": results["metadatas"][0][i],
            "distance": results["distances"][0][i]
        })

    return output

"""###To use fun"""

results = query_chroma(
    query="Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙˆØ¬Ù‡Ø© Ù„Ø°ÙˆÙŠ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ø®Ø§ØµØ©",
    db_path="./chromaDB",
    collection_name="Final",
    n_results=10,
)
##to read result

for i, r in enumerate(results, start=1):
    print(f"\n--- result {i} ---")
    print("text:", r["text"][:500], "...")
    print("source:", r["metadata"].get("source"))
    print("page:", r["metadata"].get("page"))
    print("distance:", r["distance"])

"""## ğŸ§  Step 6: Augmentation & Generation (The LLM)

We have the relevant documents. Now we construct the prompt for **openrouter**.


"""

def format_context(chunks):
    return "\n\n".join(
        f"[Source: {c['metadata'].get('source')} | Page: {c['metadata'].get('page')}]\n{c['text']}"
        for c in chunks
    )

from openai import OpenAI
import os

client = OpenAI(
    api_key=os.environ["OPENROUTER_API_KEY"],
    base_url="https://openrouter.ai/api/v1",
    default_headers={
        "HTTP-Referer": "https://colab.research.google.com/",
        "X-Title": "RAG System (Colab)"
    }
)

def rag_answer(query, retrieved_chunks):
    context = format_context(retrieved_chunks)

    response = client.chat.completions.create(
        model="openai/gpt-4o-mini",
        messages=[
            {
                "role": "system",
                 "content": (
                    "Ø£Ù†Øª Ù…Ø³Ø§Ø¹Ø¯ Ø°ÙƒÙŠ Ù…ØªØ®ØµØµ ÙÙŠ Ø§Ù„Ø¥Ø¬Ø§Ø¨Ø© Ø¹Ù† Ù…Ø­Ø§Ø¶Ø±Ø§Øª Ù…Ø¹Ø§Ù„Ø¬Ø© Ø§Ù„Ù„ØºØ§Øª Ø§Ù„Ø·Ø¨ÙŠØ¹ÙŠØ©.\n"
                    "Ø§Ù„Ù‚ÙˆØ§Ø¹Ø¯ Ø§Ù„Ù…Ù‡Ù…Ø©:\n"
                    "â€¢ Ø£Ø¬Ø¨ Ø¯Ø§Ø¦Ù…Ù‹Ø§ Ø¨Ù†ÙØ³ Ø§Ù„Ù„ØºØ© Ø§Ù„ØªÙŠ ÙƒÙØªØ¨ Ø¨Ù‡Ø§ Ø§Ù„Ø³Ø¤Ø§Ù„.\n"
                    "  - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© â†’ Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¹Ø±Ø¨ÙŠØ© Ø§Ù„ÙØµØ­Ù‰ Ø§Ù„ÙˆØ§Ø¶Ø­Ø©\n"
                    "  - Ø¥Ø°Ø§ ÙƒØ§Ù† Ø§Ù„Ø³Ø¤Ø§Ù„ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© â†’ Ø£Ø¬Ø¨ Ø¨Ø§Ù„Ø¥Ù†Ø¬Ù„ÙŠØ²ÙŠØ© Ø§Ù„ÙˆØ§Ø¶Ø­Ø©\n"
                    "â€¢ Ø§Ø³ØªØ®Ø¯Ù… Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ø§Ù„Ù…ÙˆØ¬ÙˆØ¯Ø© ÙÙŠ Ø§Ù„Ù€ Context ÙÙ‚Ø·.\n"
                    "â€¢ Ù„Ø§ ØªÙØ¶Ù Ù…Ø¹Ù„ÙˆÙ…Ø§Øª Ù…Ù† Ø®Ø§Ø±Ø¬ Ø§Ù„Ø³ÙŠØ§Ù‚.\n"
                    "â€¢ Ø¥Ø°Ø§ Ù„Ù… ÙŠÙƒÙ† Ø§Ù„Ø¬ÙˆØ§Ø¨ Ù…ÙˆØ¬ÙˆØ¯Ù‹Ø§ ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚ØŒ Ù‚Ù„: Â«Ù„Ø§ Ø£Ø¹Ø±ÙÂ» Ø£Ùˆ Â«Ù„Ø§ ØªÙˆØ¬Ø¯ Ù…Ø¹Ù„ÙˆÙ…Ø§Øª ÙƒØ§ÙÙŠØ© ÙÙŠ Ø§Ù„Ø³ÙŠØ§Ù‚Â».\n"
                    "â€¢ ÙƒÙ† Ù…ÙˆØ¬Ø²Ù‹Ø§ ÙˆØ¯Ù‚ÙŠÙ‚Ù‹Ø§ ÙˆÙ…Ø¨Ø§Ø´Ø±Ù‹Ø§."
                )
            },
            {
                "role": "user",
                "content": f"""
Context:
{context}

Question:
{query}
"""
            }
        ],
        temperature=0.2
    )

    answer = response.choices[0].message.content

    # Ø§Ø³ØªØ®Ø±Ø§Ø¬ Ø§Ù„Ù…ØµØ§Ø¯Ø± Ø§Ù„ÙØ±ÙŠØ¯Ø© Ù…Ù† retrieved_chunks
    sources = []
    seen_sources = set()  # Ù„ØªØ¬Ù†Ø¨ Ø§Ù„ØªÙƒØ±Ø§Ø±
    for i, chunk in enumerate(retrieved_chunks, start=1):
        source = chunk['metadata'].get('source', 'Unknown')
        page = chunk['metadata'].get('page', 'N/A')
        source_key = f"{source} (Page {page})"
        if source_key not in seen_sources:
            seen_sources.add(source_key)
            sources.append(f"{i}. {source_key}")

    # ØµÙŠØ§ØºØ© Ø§Ù„Ø¥Ø±Ø¬Ø§Ø¹ Ø§Ù„Ù…Ù†Ø³Ù‚
    formatted_output = f"""
ğŸ’¡ Example Output:
Student: "{query}"
Lecture-Saver 3000: "{answer}"
ğŸ” Sources:
{ '\\n'.join(sources) if sources else 'No sources found.' }
"""

    return formatted_output

"""###to use fun
arabic query
"""

query = "Ø§Ù„Ù†Ù…Ø§Ø°Ø¬ Ø§Ù„Ù…ÙˆØ¬Ù‡Ø© Ù„Ø°ÙˆÙŠ Ø§Ù„Ø§Ø­ØªÙŠØ§Ø¬Ø§Øª Ø§Ù„Ø®Ø§ØµØ©"
results = query_chroma(query, n_results=10)
formatted_answer = rag_answer(query, results)
print(formatted_answer)

"""English query"""

query = "Models designed for people with special needs"
results = query_chroma(query, n_results=10)
formatted_answer = rag_answer(query, results)
print(formatted_answer)

"""## ğŸš€ The Complete RAG Pipeline"""

new_file = "/content/Theoretical-Data-Security-Lec-1.pdf"
new_md_file = extract_pdf_to_markdown(new_file)
chunk_output_dir = "/content/NewChunks"
embed_output_dir = "/content/NewEmbeddings"

from pathlib import Path
Path(chunk_output_dir).mkdir(exist_ok=True)
Path(embed_output_dir).mkdir(exist_ok=True)

print("Starting extraction for the new file...")

split_md_files_to_json(
    md_files=[new_md_file],
    output_dir=chunk_output_dir,
    chunk_size=900,
    chunk_overlap=120
)

print(f"Chunks extracted and saved to: {chunk_output_dir}")

print("Generating embeddings...")
generate_embeddings(
    input_path=chunk_output_dir,
    output_dir=embed_output_dir,
    model_name="sentence-transformers/distiluse-base-multilingual-cased-v2"
)

print(f"Embeddings generated and saved to: {embed_output_dir}")

print("Adding new content to the database...")
collection = load_embeddings_to_chroma(
    embeddings_path=embed_output_dir,
    chroma_db_path="./chromaDB",
    collection_name="Final",
    reset_collection=False,
    peek_examples=3
)

print("New file successfully added to the collection!")

ai_query = "Ø£Ù‡Ø¯Ø§Ù Ø£Ù…Ù† Ø§Ù„Ù…Ø¹Ù„ÙˆÙ…Ø§Øª"
results = query_chroma(
    query=ai_query,
    db_path="./chromaDB",
    collection_name="Final",
    n_results=10
)

formatted_answer = rag_answer(ai_query, results)
print("\n" + "="*70)
print("Query:", ai_query)
print("Answer:", formatted_answer)
print("="*70)

import zipfile
from pathlib import Path

zip_file = "/content/chromaDB (1).zip"
extract_dir = "/content/chromaDB"  # Ø§Ù„Ù…Ø¬Ù„Ø¯ Ø§Ù„Ø°ÙŠ Ø³ÙŠØªÙ… Ø§Ù„Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙÙŠÙ‡

Path(extract_dir).mkdir(exist_ok=True)

with zipfile.ZipFile(zip_file, 'r') as zip_ref:
    zip_ref.extractall(extract_dir)

print(f"ØªÙ… ÙÙƒ Ø¶ØºØ· Ø§Ù„Ù…Ù„ÙØ§Øª ÙÙŠ: {extract_dir}")

